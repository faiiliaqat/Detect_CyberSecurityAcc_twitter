{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adac17fd",
   "metadata": {},
   "source": [
    "# Supervised Learning: Classification\n",
    "\n",
    "### Acknowledgements: Usman Alim \n",
    "\n",
    "\n",
    "\n",
    "Further Reading:\n",
    "\n",
    "* `scikit-learn`: [user guide](https://scikit-learn.org/stable/user_guide.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e954fb",
   "metadata": {},
   "source": [
    "## Quick Overview of scikit-learn\n",
    "\n",
    "- [`scikit-learn`](https://scikit-learn.org/stable/) is the main machine learning library in the Python data science ecosystem.\n",
    "- Implements many supervised (classification, regression) and unsupervised (clustering, density estimation, dimensionality reduction) learning algorithms.\n",
    "- Relies heavily on `numpy`. Inputs and outputs are numpy arrays.\n",
    "- Input data are expected to be $n \\times D$ numerical arrays where $n$ is the number of observations, and $D$ is the number of features.\n",
    "- Some feature wrangling may be needed, provides methods for feature extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_data['Tweet'] = data['Tweet'].str.replace('[^a-zA-Z]', ' ')\n",
    "# p_data\n",
    "\n",
    "# p_data['Tweet'] = p_data['Tweet'].apply(lambda x: x.lower())\n",
    "# p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3771e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import SnowballStemmer\n",
    "# import string\n",
    "\n",
    "# stopwords = set(stopwords.words('english'))\n",
    "# stemmer = SnowballStemmer('english')\n",
    "# # data['Tweet'] = p_data['Tweet'].apply(word_tokenize)\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Remove punctuation\n",
    "#     text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "#     # Convert to lowercase\n",
    "#     text = text.lower()\n",
    "    \n",
    "#     # Remove stopwords and stem words\n",
    "#     words = [stemmer.stem(word) for word in text.split() if word not in stopwords]\n",
    "#     return ' '.join(words)\n",
    "\n",
    "# p_data['preprocessed_text'] = p_data['Tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data processing related imports\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# model related imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the dataset from the url into pandas dataframe\n",
    "# url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "# names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'label']\n",
    "data = pd.read_csv('train.csv')\n",
    "display(data)\n",
    "data = data.drop(data.columns[7], axis=1)\n",
    "data = data[data['Type'] != 'South Dakota']\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbfc7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame\n",
    "p_data = pd.DataFrame(columns=['Tweet', 'following', 'followers', 'actions', 'is_retweet', 'location', 'Type'])\n",
    "p_data = p_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bcf8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data for model\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "def preprocess_text(raw_tweet):\n",
    "    # Remove punctuation\n",
    "    new_string = []\n",
    "    for word in raw_tweet:\n",
    "        # Check if the word is a URL\n",
    "        if \"https://\" in word or \"http://\" in word:\n",
    "            new_string.append(word)\n",
    "        else:\n",
    "            word = re.sub(r\"[^A-Za-z0-9.\\-]\", \" \", word)\n",
    "            new_string.append(word)\n",
    "\n",
    "    # Join the list of words into a string\n",
    "    string1 = \"\".join(new_string)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    string1 = re.sub(r\"\\s+\", \" \", string1)\n",
    "    \n",
    "#     # Remove leading and trailing spaces\n",
    "#     string1 = string1.strip()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    string1 = string1.lower()\n",
    "\n",
    "    return string1\n",
    "\n",
    "# p_data['preprocessed_text'] = p_data['Tweet'].apply(preprocess_text)\n",
    "# p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data['preprocessed_text'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature('Tweet') and lable ('Type') data from a Training and Test dataset\n",
    "# We use 80% of the dataset to train a model, and use the rest to test the model predictions\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(p_data['preprocessed_text'], \n",
    "#                                                     p_data['Type'], test_size=.20, random_state=70)\n",
    "\n",
    "# random_state = 70 gave us the best accuracy so far. tried varies mubers like  30, 50, 60, 80, 90\n",
    "\n",
    "# splitting data and writing into csv file for consistent outputs. \n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test = train_test_split(p_data, test_size=.20, random_state=70)\n",
    "\n",
    "# X_train.to_csv('train80.csv', index=False)\n",
    "# X_test.to_csv('test20.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d7d0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "990e1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geopy.geocoders import Nominatim\n",
    "\n",
    "# # create a geocoder object\n",
    "# geolocator = Nominatim(user_agent='my-app')\n",
    "\n",
    "# # define a function to extract country names\n",
    "# def get_country(location):\n",
    "    \n",
    "#     print(\"starting\")\n",
    "#     try:\n",
    "#         # use geocoder to get location information\n",
    "#         loc = geolocator.geocode(location)\n",
    "#         # extract country name from location\n",
    "#         country = loc.raw['address']['country']\n",
    "#         return country\n",
    "#     except:\n",
    "#         # if location information is not available, return None\n",
    "#         return \"unKnown location\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51859b05",
   "metadata": {},
   "source": [
    "#Start from here:::::::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9d2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "328cdb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c_vec_test_train(train_feature, test_feature,c_vec):\n",
    "\n",
    "    # Fit and transform the preprocessed text\n",
    "    c_vec_train = c_vec.fit_transform(train_feature)\n",
    "    c_vect_test = c_vec.transform(test_feature)\n",
    "    \n",
    "    return c_vec_train, c_vect_test\n",
    "\n",
    "def get_tf_vec_test_train(train_feature, test_feature, tfid_vec):\n",
    "\n",
    "    tf_vec_train = tfid_vec.fit_transform(train_feature)\n",
    "    tf_vec_test = tfid_vec.transform(test_feature)\n",
    "\n",
    "    return tf_vec_train, tf_vec_test\n",
    "\n",
    "def get_scalar_test_train(train_feature, test_feature, scaler ):\n",
    "    train_feature = np.array(train_feature)\n",
    "    test_feature = np.array(test_feature)\n",
    "    \n",
    "    train_feature = train_feature.reshape(-1, 1)\n",
    "    test_feature = test_feature.reshape(-1, 1)\n",
    "    \n",
    "    train_scaled = scaler.fit_transform(train_feature)\n",
    "    test_scaled = scaler.transform(test_feature)\n",
    "    \n",
    "    return  train_scaled, test_scaled\n",
    "    \n",
    "                                    \n",
    "def get_NB(train_feature, test_feature, v_type, vectorizor, model):\n",
    "     \n",
    "    y_train = train_feature\n",
    "    y_test = test_feature\n",
    "    vectype = v_type\n",
    "    \n",
    "    if (vectype == \"Count_Vectorizer\"):\n",
    "        vec_train, vect_test = get_c_vec_test_train(train_feature, test_feature, vectorizor)\n",
    "    elif(vectype == \"tfid_Vectorizer\"):\n",
    "        vec_train, vect_test =  get_tf_vec_test_train(train_feature, test_feature, vectorizor)\n",
    "                                   \n",
    "    else:\n",
    "        print(\"Error with vector type\")\n",
    "     \n",
    "        \n",
    "     # Train a Naive Bayes classifier on the count vectorized text\n",
    "    model.fit(vec_train, y_train)\n",
    "\n",
    "    # Use the trained classifier to make predictions on the vectorized new text data\n",
    "    pred_y = model.predict(vect_test)\n",
    "    \n",
    "    \n",
    "    # Compute the accuracy score of the predicted labels\n",
    "    accuracy = accuracy_score(y_test, pred_y)\n",
    "    print(\"accuracy by NB using {} vectorizer: {:.4f}%\".format(vectype, accuracy * 100))\n",
    "    return pred_y\n",
    "  \n",
    "    \n",
    "def get_SVC(train_feature, test_feature, v_type, vectorizor, model):\n",
    "    \n",
    "    y_train = train_feature\n",
    "    y_test = test_feature\n",
    "    \n",
    "    vectype = v_type\n",
    "    \n",
    "    if (vectype == \"Count_Vectorizer\"):\n",
    "        vec_train, vect_test = get_c_vec_test_train(train_feature, test_feature,vectorizor)    \n",
    "    elif(vectype == \"tfid_Vectorizer\"):\n",
    "        vec_train, vect_test =  get_tf_vec_test_train(train_feature, test_feature, vectorizor)\n",
    "    else:\n",
    "        print(\"Error with vector type\")\n",
    "    \n",
    "\n",
    "    # Train SVM model using Tf vector\n",
    "\n",
    "    clf.fit(vec_train, y_train)\n",
    "\n",
    "    pred_y = clf.predict(vect_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    accuracy = accuracy_score(y_test, pred_y)\n",
    "    print(\"accuracy by SVC using {} vectorizer: {:.4f}%\".format( vectype, accuracy * 100))\n",
    "\n",
    "    return pred_y\n",
    "\n",
    "\n",
    "# def get_ConfusionMatrix(model, pred_y):\n",
    "    \n",
    "#     y_test = test_data['Type']\n",
    "        \n",
    "\n",
    "#     cm = confusion_matrix(y_test, pred_y)\n",
    "\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "#     disp.plot()\n",
    "\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "# def get_ConMatrix_text(train_data, test_data):\n",
    "\n",
    "#     get_ConfusionMatrix(nb, \n",
    "#                         get_NB(train_data, test_data,\"tfid_vec\", tfid_vec, nb) )\n",
    "\n",
    "#     get_ConfusionMatrix(clf,\n",
    "#                         get_SVC(train_data, test_data,\"tfid_vec\", tfid_vec, clf) )\n",
    "    \n",
    "#     get_ConfusionMatrix(nb, \n",
    "#                     get_NB(train_data, test_data,\"c_vec\", c_vec, nb) )\n",
    "\n",
    "#     get_ConfusionMatrix(clf,\n",
    "#                     get_SVC(train_data, test_data,\"c_vec\", c_vec, clf) )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9be084c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ConfusionMatrix(model, test_data ,pred_y, ax, label):\n",
    "    y_test = test_data\n",
    "    cm = confusion_matrix(y_test, pred_y)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=None)\n",
    "    disp.plot(ax=ax)\n",
    "    ax.set_title(label)\n",
    "\n",
    "\n",
    "def print_CMatrix(train_data, test_data, feature):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    labels = ['TF-ID NB', 'Count NB', 'TF-ID SVM', 'Count SVM']\n",
    "    classifiers = [nb, nb, clf,  clf]\n",
    "    vectorizers = [tfid_vec, c_vec, tfid_vec, c_vec]\n",
    "    vectorizer_labels = ['tfid_Vectorizer', 'Count_Vectorizer', 'tfid_Vectorizer', 'Count_Vectorizer']\n",
    "    all_labels = set(test_data)\n",
    "    for i in range(4):\n",
    "        pred_y = get_NB(train_data, test_data, vectorizer_labels[i], vectorizers[i], classifiers[i]) if i < 2 else get_SVC(train_data, test_data, vectorizer_labels[i], vectorizers[i], classifiers[i])\n",
    "        get_ConfusionMatrix(classifiers[i], test_data ,  pred_y, axs[i], labels[i])\n",
    "\n",
    "    s = 'Confusion Matrices of ' + feature \n",
    "    fig.suptitle(s)\n",
    "    handles, labels = axs[-1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=1)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "804e8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train80.csv')\n",
    "test_data = pd.read_csv('test20.csv')\n",
    "\n",
    "\n",
    "train_data['followers'] = train_data['followers'].fillna(-1)\n",
    "test_data['followers'] = test_data['followers'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c3267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Naive Bayes classifier \n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Defining SVC model \n",
    "clf = SVC(kernel='linear', C=1, gamma='auto')\n",
    "\n",
    "# Initialize the vectorizer with desired parameters\n",
    "c_vec = CountVectorizer(max_features=1000)\n",
    "\n",
    "# Vectorize tweets using TfidfVectorizer\n",
    "tfid_vec = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ac1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_numbers(df_num_col):\n",
    "\n",
    "    # Convert the numbers to a string representation\n",
    "    data_str = [\"{:.4f}\".format(x) for x in df_num_col]\n",
    "    # document = \" \".join(data_str)\n",
    "    processed_num_list = data_str\n",
    "    return processed_num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f073d854",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\faii_\\OneDrive - University of Calgary\\UoC\\WN2023\\571-DESKTOP-RMD0HT3\\project571\\Supervised Learning example2.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/faii_/OneDrive%20-%20University%20of%20Calgary/UoC/WN2023/571-DESKTOP-RMD0HT3/project571/Supervised%20Learning%20example2.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_follower \u001b[39m=\u001b[39m preprocess_numbers(train_data[\u001b[39m'\u001b[39m\u001b[39mfollowers\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/faii_/OneDrive%20-%20University%20of%20Calgary/UoC/WN2023/571-DESKTOP-RMD0HT3/project571/Supervised%20Learning%20example2.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_follower \u001b[39m=\u001b[39m preprocess_numbers(test_data[\u001b[39m'\u001b[39m\u001b[39mfollowers\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_follower = preprocess_numbers(train_data['followers'])\n",
    "test_follower = preprocess_numbers(test_data['followers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a9aa7",
   "metadata": {},
   "source": [
    "# Visualizing the results\n",
    "\n",
    "- We can visualize the results using a confusion matrix\n",
    "- The confusion matrix tells us exactly how many of each type of flowers the model predicted and what the correct answers are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e57135",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_train = train_data['preprocessed_text']\n",
    "tweet_test = test_data['preprocessed_text']\n",
    "\n",
    "# print_CMatrix(tweet_train, tweet_test,\"Tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea838ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_CMatrix_text(train_follower, test_follower, \"followers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f679da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "with open('train80loc.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    \n",
    "train_data_loc = pd.read_csv('train80loc.csv', encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3eb61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test20loc.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    \n",
    "test_data_loc = pd.read_csv('test20loc.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee5f7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to a pandas dataframe column\n",
    "train_data_loc = train_data_loc['location'].apply(preprocess_text)\n",
    "test_data_loc = test_data_loc['location'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11d80e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate two features\n",
    "con_loc_train = tweet_train + ' ' + train_data_loc\n",
    "con_loc_test = tweet_test + ' ' + test_data_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "617bfb23",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.05 GiB for an array with shape (140620600,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprint_CMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcon_loc_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon_loc_test\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mprint_CMatrix\u001b[1;34m(train_data, test_data, feature)\u001b[0m\n\u001b[0;32m     15\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(test_data)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     pred_y \u001b[38;5;241m=\u001b[39m \u001b[43mget_NB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifiers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m get_SVC(train_data, test_data, vectorizer_labels[i], vectorizers[i], classifiers[i])\n\u001b[0;32m     18\u001b[0m     get_ConfusionMatrix(classifiers[i], test_data ,  pred_y, axs[i], labels[i])\n\u001b[0;32m     20\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfusion Matrices of \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m feature \n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mget_NB\u001b[1;34m(train_feature, test_feature, v_type, vectorizor, model)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError with vector type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m  \u001b[38;5;66;03m# Train a Naive Bayes classifier on the count vectorized text\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvec_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Use the trained classifier to make predictions on the vectorized new text data\u001b[39;00m\n\u001b[0;32m     48\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(vect_test)\n",
      "File \u001b[1;32mC:\\ProgramFiles\\lib\\site-packages\\sklearn\\naive_bayes.py:690\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    688\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[1;32m--> 690\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n",
      "File \u001b[1;32mC:\\ProgramFiles\\lib\\site-packages\\sklearn\\naive_bayes.py:864\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[0;32m    863\u001b[0m check_non_negative(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultinomialNB (input X)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 864\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramFiles\\lib\\site-packages\\sklearn\\utils\\extmath.py:153\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    156\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m ):\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\sparse\\_base.py:636\u001b[0m, in \u001b[0;36mspmatrix.__rmatmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    634\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScalar operands are not allowed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    635\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rmul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\sparse\\_base.py:614\u001b[0m, in \u001b[0;36mspmatrix._rmul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    613\u001b[0m     tr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(other)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m--> 614\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\sparse\\_base.py:532\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_vector(other\u001b[38;5;241m.\u001b[39mravel())\u001b[38;5;241m.\u001b[39mreshape(M, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m other\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m N:\n\u001b[1;32m--> 532\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_multivector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_scalar(other)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\sparse\\_compressed.py:502\u001b[0m, in \u001b[0;36m_cs_matrix._mul_multivector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# csr_matvecs or csc_matvecs\u001b[39;00m\n\u001b[0;32m    501\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_matvecs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 502\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_vecs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m   \u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.05 GiB for an array with shape (140620600,) and data type float64"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_CMatrix(con_loc_train, con_loc_test,\"location\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
