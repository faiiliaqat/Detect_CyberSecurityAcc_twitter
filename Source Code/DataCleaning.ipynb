{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adac17fd",
   "metadata": {},
   "source": [
    "# Supervised Learning: Classification\n",
    "\n",
    "### Acknowledgements: Usman Alim \n",
    "\n",
    "\n",
    "\n",
    "Further Reading:\n",
    "\n",
    "* `scikit-learn`: [user guide](https://scikit-learn.org/stable/user_guide.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e954fb",
   "metadata": {},
   "source": [
    "## Quick Overview of scikit-learn\n",
    "\n",
    "- [`scikit-learn`](https://scikit-learn.org/stable/) is the main machine learning library in the Python data science ecosystem.\n",
    "- Implements many supervised (classification, regression) and unsupervised (clustering, density estimation, dimensionality reduction) learning algorithms.\n",
    "- Relies heavily on `numpy`. Inputs and outputs are numpy arrays.\n",
    "- Input data are expected to be $n \\times D$ numerical arrays where $n$ is the number of observations, and $D$ is the number of features.\n",
    "- Some feature wrangling may be needed, provides methods for feature extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22c768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data processing related imports\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# model related imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data for model\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "def preprocess_text(raw_tweet):\n",
    "    # Remove punctuation\n",
    "    new_string = []\n",
    "    for word in raw_tweet:\n",
    "        # Check if the word is a URL\n",
    "        if \"https://\" in word or \"http://\" in word:\n",
    "            new_string.append(word)\n",
    "        else:\n",
    "            word = re.sub(r\"[^A-Za-z0-9.\\-]\", \" \", word)\n",
    "            new_string.append(word)\n",
    "\n",
    "    # Join the list of words into a string\n",
    "    string1 = \"\".join(new_string)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    string1 = re.sub(r\"\\s+\", \" \", string1)\n",
    "    \n",
    "#     # Remove leading and trailing spaces\n",
    "#     string1 = string1.strip()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    string1 = string1.lower()\n",
    "\n",
    "    return string1\n",
    "\n",
    "p_data['preprocessed_text'] = p_data['Tweet'].apply(preprocess_text)\n",
    "p_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "804e8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train80.csv')\n",
    "test_data = pd.read_csv('test20.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe4ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['followers'] = train_data['followers'].fillna(-1)\n",
    "test_data['followers'] = test_data['followers'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get the feature('Tweet') and lable ('Type') data from a Training and Test dataset\n",
    "We use 80% of the dataset to train a model, and use the rest to test the model predictions\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(p_data['preprocessed_text'], \n",
    "                                                    p_data['Type'], test_size=.20, random_state=70)\n",
    "\n",
    "# random_state = 70 gave us the best accuracy so far. tried varies mubers like  30, 50, 60, 80, 90\n",
    "\n",
    "# splitting data and writing into csv file for consistent outputs. \n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test = train_test_split(p_data, test_size=.20, random_state=70)\n",
    "\n",
    "X_train.to_csv('train80.csv', index=False)\n",
    "X_test.to_csv('test20.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51859b05",
   "metadata": {},
   "source": [
    "#Start from here:::::::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "328cdb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c_vec_test_train(train_feature, test_feature,c_vec):\n",
    "\n",
    "    # Fit and transform the preprocessed text\n",
    "    c_vec_train = c_vec.fit_transform(train_feature)\n",
    "    c_vect_test = c_vec.transform(test_feature)\n",
    "    \n",
    "    return c_vec_train, c_vect_test\n",
    "\n",
    "def get_tf_vec_test_train(train_feature, test_feature, tfid_vec):\n",
    "\n",
    "    tf_vec_train = tfid_vec.fit_transform(train_feature)\n",
    "    tf_vec_test = tfid_vec.transform(test_feature)\n",
    "\n",
    "    return tf_vec_train, tf_vec_test\n",
    "\n",
    "def get_scalar_test_train(train_feature, test_feature, scaler ):\n",
    "    train_feature = np.array(train_feature)\n",
    "    test_feature = np.array(test_feature)\n",
    "    \n",
    "    train_feature = train_feature.reshape(-1, 1)\n",
    "    test_feature = test_feature.reshape(-1, 1)\n",
    "    \n",
    "    train_scaled = scaler.fit_transform(train_feature)\n",
    "    test_scaled = scaler.transform(test_feature)\n",
    "    \n",
    "    return  train_scaled, test_scaled\n",
    "    \n",
    "                                    \n",
    "def get_NB(train_feature, test_feature, v_type, vectorizor, model):\n",
    "     \n",
    "    y_train = train_data['Type']\n",
    "    y_test = test_data['Type']\n",
    "    vectype = v_type\n",
    "    \n",
    "    vec_train = train_feature\n",
    "    vect_test  = test_feature\n",
    "    \n",
    "    if (vectype == \"c_vec\"):\n",
    "        vec_train, vect_test = get_c_vec_test_train(train_feature, test_feature, vectorizor)\n",
    "    elif(vectype == \"tfid_vec\"):\n",
    "        vec_train, vect_test =  get_tf_vec_test_train(train_feature, test_feature, vectorizor)                         \n",
    "    else:\n",
    "        print(\"Error with vector type\")\n",
    "     \n",
    "        \n",
    "     # Train a Naive Bayes classifier on the count vectorized text\n",
    "    model.fit(vec_train, y_train)\n",
    "\n",
    "    # Use the trained classifier to make predictions on the vectorized new text data\n",
    "    pred_y = model.predict(vect_test)\n",
    "    \n",
    "    \n",
    "    # Compute the accuracy score of the predicted labels\n",
    "    accuracy = accuracy_score(y_test, pred_y)\n",
    "    print(\"accuracy by NB using {} vectorizer: {:.4f}%\".format(vectype, accuracy * 100))\n",
    "    return pred_y\n",
    "  \n",
    "    \n",
    "def get_SVC(train_feature, test_feature, v_type, vectorizor, model):\n",
    "    \n",
    "    y_train = train_data['Type']\n",
    "    y_test = test_data['Type']\n",
    "    \n",
    "    vectype = v_type\n",
    "    \n",
    "    if (vectype == \"c_vec\"):\n",
    "        vec_train, vect_test = get_c_vec_test_train(train_feature, test_feature,vectorizor)\n",
    "        \n",
    "    elif(vectype == \"tfid_vec\"):\n",
    "        vec_train, vect_test =  get_tf_vec_test_train(train_feature, test_feature, vectorizor)\n",
    "    \n",
    "    elif(vectype == \"scaler\"):\n",
    "        vec_train, vect_test =  get_scalar_test_train(train_feature, test_feature, vectorizor)\n",
    "    else:\n",
    "        print(\"Error with vector type\")\n",
    "    \n",
    "\n",
    "    # Train SVM model using Tf vector\n",
    "\n",
    "    clf.fit(vec_train, y_train)\n",
    "\n",
    "    pred_y = clf.predict(vect_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    accuracy = accuracy_score(y_test, pred_y)\n",
    "    print(\"accuracy by SVC using {} vectorizer: {:.4f}%\".format( vectype, accuracy * 100))\n",
    "\n",
    "    return pred_y\n",
    "\n",
    "\n",
    "def get_ConfusionMatrix(model, pred_y):\n",
    "    \n",
    "    y_test = test_data['Type']\n",
    "        \n",
    "\n",
    "    cm = confusion_matrix(y_test, pred_y)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def get_ConMatrix_text(train_data, test_data):\n",
    "\n",
    "    get_ConfusionMatrix(nb, \n",
    "                        get_NB(train_data, test_data,\"tfid_vec\", tfid_vec, nb) )\n",
    "\n",
    "    get_ConfusionMatrix(clf,\n",
    "                        get_SVC(train_data, test_data,\"tfid_vec\", tfid_vec, clf) )\n",
    "    \n",
    "    get_ConfusionMatrix(nb, \n",
    "                    get_NB(train_data, test_data,\"c_vec\", c_vec, nb) )\n",
    "\n",
    "    get_ConfusionMatrix(clf,\n",
    "                    get_SVC(train_data, test_data,\"c_vec\", c_vec, clf) )\n",
    "    \n",
    "    \n",
    "def get_ConMatrix_num(train_data, test_data ):\n",
    "\n",
    "    get_ConfusionMatrix(nb, \n",
    "                        get_NB(train_data, test_data,\"\", scaler, nb) )\n",
    "\n",
    "    get_ConfusionMatrix(clf,\n",
    "                        get_SVC(train_data, test_data,\"scaler\", scaler, clf) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c3267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Naive Bayes classifier \n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Defining SVC model \n",
    "clf = SVC(kernel='linear', C=1, gamma='auto')\n",
    "\n",
    "# Initialize the vectorizer with desired parameters\n",
    "c_vec = CountVectorizer(max_features=1000)\n",
    "\n",
    "# Vectorize tweets using TfidfVectorizer\n",
    "tfid_vec = TfidfVectorizer()\n",
    "\n",
    "# initialize the scaler \n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ac1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_numbers(df_num_col):\n",
    "\n",
    "    # Convert the numbers to a string representation\n",
    "    data_str = [\"{:.4f}\".format(x) for x in df_num_col]\n",
    "    # document = \" \".join(data_str)\n",
    "    processed_num_list = data_str\n",
    "    return processed_num_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
